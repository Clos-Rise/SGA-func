# SGA (Smooth Gaussian Activation)

Простая , легкая , функция активации для нейронных сетей. Выглядит следующиим образом : g(x)=x⋅e^−αx2.

Для Python :

<pre> python import numpy as np
 def SGA(x, alpha=0.1):
          return x * np.exp(-alpha * x**2) </pre>

Простенькие Тесты :

      x      SGA   ReLU  LeakyReLU       ELU      Tanh
0  -3.0 -1.219709   0.0     -0.030 -0.950213 -0.995055
1  -2.5 -1.338154   0.0     -0.025 -0.917915 -0.986614
2  -2.0 -1.340640   0.0     -0.020 -0.864665 -0.964028
3  -1.5 -1.197774   0.0     -0.015 -0.776870 -0.905148
4  -1.0 -0.904837   0.0     -0.010 -0.632121 -0.761594
5  -0.5 -0.487655   0.0     -0.005 -0.393469 -0.462117
6   0.0  0.000000   0.0      0.000  0.000000  0.000000
7   0.5  0.487655   0.5      0.500  0.500000  0.462117
8   1.0  0.904837   1.0      1.000  1.000000  0.761594
9   1.5  1.197774   1.5      1.500  1.500000  0.905148
10  2.0  1.340640   2.0      2.000  2.000000  0.964028
11  2.5  1.338154   2.5      2.500  2.500000  0.986614
12  3.0  1.219709   3.0      3.000  3.000000  0.995055
