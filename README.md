# SGA (Smooth Gaussian Activation)

Простая , легкая , функция активации для нейронных сетей. Выглядит следующиим образом : g(x)=x⋅e^−αx2.

Для Python :

<pre> python import numpy as np
 def SGA(x, alpha=0.1):
          return x * np.exp(-alpha * x**2) </pre>

Простенькие Тесты :

| x   |  SGA    | ReLU | LeakyReLU |   ELU    |  Tanh   |
|-----|---------|------|-----------|----------|---------|
| -3.0| -1.2197 | 0.0  | -0.030    | -0.9502  | -0.9951 |
| -2.5| -1.3382 | 0.0  | -0.025    | -0.9179  | -0.9866 |
| -2.0| -1.3406 | 0.0  | -0.020    | -0.8647  | -0.9640 |
| -1.5| -1.1978 | 0.0  | -0.015    | -0.7769  | -0.9051 |
| -1.0| -0.9048 | 0.0  | -0.010    | -0.6321  | -0.7616 |
| -0.5| -0.4877 | 0.0  | -0.005    | -0.3935  | -0.4621 |
|  0.0|  0.0000 | 0.0  |  0.000    |  0.0000  |  0.0000 |
|  0.5|  0.4877 | 0.5  |  0.500    |  0.5000  |  0.4621 |
|  1.0|  0.9048 | 1.0  |  1.000    |  1.0000  |  0.7616 |
|  1.5|  1.1978 | 1.5  |  1.500    |  1.5000  |  0.9051 |
|  2.0|  1.3406 | 2.0  |  2.000    |  2.0000  |  0.9640 |
|  2.5|  1.3382 | 2.5  |  2.500    |  2.5000  |  0.9866 |
|  3.0|  1.2197 | 3.0  |  3.000    |  3.0000  |  0.9951 |

